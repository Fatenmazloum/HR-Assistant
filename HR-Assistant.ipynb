{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22ad8d5",
   "metadata": {},
   "source": [
    "Langchain  is a python framework that wires multiple steps together while building an LLM application:\n",
    "\n",
    "Components:\n",
    "\n",
    "1. LLM: The language model you use to generate answers like a human and to understand the question.\n",
    "\n",
    "2. Tools: These allow the LLM to call functions (tools) when needed via agents.\n",
    "\n",
    "3. Agents: A smart system that knows which tool should be used.\n",
    "\n",
    "4. vector database: perform semantic search for retrieving relevant documents or information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9920ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df86191",
   "metadata": {},
   "source": [
    "LangChain runs your app — LangSmith watches and explains how. Gives you an overall view of the full process inside your LangChain application:\n",
    "\n",
    "User Input\n",
    "See exactly what the user asked.\n",
    "\n",
    "LLM Interpretation\n",
    "How the language model understood the question.\n",
    "\n",
    "Agent Decisions\n",
    "Which tool the agent chose to use (and why).\n",
    "\n",
    "Tool Calls\n",
    "What tools were called, with what parameters (e.g., employee_id = \"123\").\n",
    "\n",
    "Tool Responses\n",
    "The data returned from each tool/function.\n",
    "\n",
    "Final LLM Response\n",
    "The final answer generated using the tool's result.\n",
    "\n",
    "Execution Timeline\n",
    "Visual flow of steps: input → tool → output → answer.\n",
    "\n",
    "🧪 Bonus: LangSmith Also Helps With\n",
    "Debugging errors\n",
    "\n",
    "Testing prompts or tool logic\n",
    "\n",
    "Improving agent reasoning\n",
    "\n",
    "Comparing different models or chain designs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a96bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN_API_KEY\" #get from .env api key for langsmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"HR assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ed090",
   "metadata": {},
   "source": [
    "1. Tools:\n",
    "\n",
    "Are functions or external capabilities (e.g., databases, APIs).\n",
    "\n",
    "Perform specific tasks like querying a database, making a calculation, or generating content.\n",
    "\n",
    "Do not decide which task to perform. They simply execute the action they're called to do.\n",
    "\n",
    "2. Agents:\n",
    "\n",
    "Are decision-makers.\n",
    "\n",
    "Decide when and which tools to call based on the input (query) they receive.\n",
    "\n",
    "They orchestrate the entire process of interacting with tools to achieve a goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaef18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "apikey = os.getenv('OPENAI_API_KEY')#load api key from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a84f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM (OpenAI gpt-3.5-turbo)\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', api_key=apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions:\n",
    "def get_employee_name(id: str) -> dict:\n",
    "    if id == '123':\n",
    "        return {'name': 'Smith', 'role': 'Manager', 'Department': 'IT'}\n",
    "    elif id == '453':\n",
    "        return {'name': 'Paul', 'role': 'Supervisor', 'Department': 'CCE'}\n",
    "    else:\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3373af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_leave_balance(id: str) -> dict:\n",
    "    if id == '123':\n",
    "        return {'remaining_days': '10', 'used_days': '30'}\n",
    "    elif id == '453':\n",
    "        return {'remaining_days': '2', 'used_days': '38'}\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interview_question(role: str) -> list:\n",
    "    if role == 'Computer Engineering':\n",
    "        return [\n",
    "            \"Can you explain the differences between a process and a thread?\",\n",
    "            \"How does a CPU handle interrupts, and why are they important?\",\n",
    "            \"Describe how virtual memory works and why it's used.\",\n",
    "            \"What are the key differences between RISC and CISC architectures?\",\n",
    "            \"How would you optimize code for performance at the hardware level?\"\n",
    "        ]\n",
    "    elif role == 'AI Engineering':\n",
    "        return [\n",
    "            \"Explain the difference between supervised, unsupervised, and reinforcement learning.\",\n",
    "            \"How do you handle imbalanced datasets in machine learning?\",\n",
    "            \"What is the bias-variance tradeoff?\",\n",
    "            \"Describe a project where you deployed a machine learning model in production.\",\n",
    "            \"How would you select the right model for a classification problem?\"\n",
    "        ]\n",
    "    elif role == 'Biomedical Engineering':\n",
    "        return [\n",
    "            \"What biomedical devices have you worked with, and what challenges did you face?\",\n",
    "            \"Explain how you would validate the accuracy and safety of a medical sensor.\",\n",
    "            \"Describe a time you collaborated with clinicians or patients during a project.\",\n",
    "            \"What is your experience with regulatory compliance in biomedical design?\",\n",
    "            \"How would you improve the design of a prosthetic limb for better user comfort?\"\n",
    "        ]\n",
    "    else:\n",
    "        return [\"No questions available for this role.\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59fbc12",
   "metadata": {},
   "source": [
    "🟢 Use with type hints (def check_leave_balance(id: str) -> dict: instead of def check_leave_balance(id) )for: \n",
    "\n",
    "Better code clarity\n",
    "\n",
    "Fewer bugs\n",
    "\n",
    "Improved editor support (autocomplete, linting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tools\n",
    "from langchain.tools import Tool\n",
    "\n",
    "emp_name_tool = Tool.from_function(\n",
    "    name='check_employee_information',\n",
    "    func=get_employee_name,\n",
    "    description=\"Use this tool to retrieve an employee's name, role, and department by providing their employee ID.\"\n",
    ")\n",
    "\n",
    "leave_balance_tool = Tool.from_function(\n",
    "    name='check_leave_balance',\n",
    "    func=check_leave_balance,\n",
    "    description='Use this tool to check how many leave days an employee has by providing the employee ID.'\n",
    ")\n",
    "\n",
    "interview_tool = Tool.from_function(\n",
    "    name='generate_interview_questions',\n",
    "    func=generate_interview_question,\n",
    "    description='Use this tool to generate interview questions for a given job role.'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f43121",
   "metadata": {},
   "source": [
    "Tool key parameters:\n",
    "\n",
    "1. name\t: The name of the tool \n",
    "2. func:  The Python function to execute without ()\n",
    "3. description:\tA description of what the tool does (helps the agent decide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c764b",
   "metadata": {},
   "source": [
    "Agent(decide what tool to use) parameters:\n",
    "\n",
    "1. tools: A list of tools it can use\n",
    "\n",
    "2. llm: The LLM (Language Model)  is used to understand the question and generate a human-like answer.\n",
    "\n",
    "3. agent_type: The logic type (e.g., ZERO_SHOT_REACT_DESCRIPTION)\n",
    "\n",
    "4. verbose: If True, shows step-by-step reasoning in the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[emp_name_tool, leave_balance_tool, interview_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,#use the description to indicate which tool to be used\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f008e",
   "metadata": {},
   "source": [
    "How the Process Works:\n",
    "\n",
    "User Query: The user asks a question (e.g., \"What is my leave balance?\").\n",
    "\n",
    "LLM Processing:\n",
    "\n",
    "The LLM inside the agent analyzes the query and understands that the user wants to know their leave balance.\n",
    "\n",
    "The LLM decides whether to directly answer the question or call an external tool.\n",
    "\n",
    "Tool Call (If Needed): If the LLM decides that a tool (e.g., check_leave_balance) is needed to fetch data, it formats the call and passes the necessary parameters (e.g., employee ID).\n",
    "\n",
    "Tool Response: The tool executes (e.g., querying the database) and returns the required information (e.g., \"You have 10 days of leave remaining\").\n",
    "\n",
    "LLM Response Generation:\n",
    "\n",
    "The LLM formats the result into a clear and human-like answer.\n",
    "\n",
    "For instance, \"Your remaining leave balance is 10 days.\"\n",
    "\n",
    "Final Answer: The agent sends the response back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375610be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st \n",
    "st.set_page_config(page_title='HR-Assistant',layout='centered')\n",
    "st.title('Hello Iam your HR assistnt')\n",
    "question=st.text_input('Enter your question')\n",
    "if question:\n",
    "    response = agent.run(question)\n",
    "    st.write(response)\n",
    "else:\n",
    "    st.write(\"Please enter a question.\")\n",
    "\n",
    "#Don't use context[0].message.content when you're using LangChain’s built-in tools, agents, or chains, because LangChain automatically handles the message processing for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e822044",
   "metadata": {},
   "source": [
    "✅ If the answer to a question is not available through any predefined functions (tools), and the answer needs to be generated from the LLM (like GPT) itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1835b",
   "metadata": {},
   "source": [
    "langchain.llms: Use for simple one-time questions(from langchain.llms import OpenAI...OpenAI(model=,apikey=))\n",
    "\n",
    "langchain-community.llms: Use for community or third-party LLMs (Hugging face)\n",
    "\n",
    "langchain.chatmodels: Use for chat models that remember the conversation(fromlangchain.chatmodelsimport ChatOpenAi..... ChatOpenAI(model=,apikey=..) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c329fe",
   "metadata": {},
   "source": [
    "✅ Summary: How LangChain Extracts ID from Natural Language\n",
    "When you type something like:\n",
    "\n",
    "\"Check for me leave balance from employee with id 123\"\n",
    "\n",
    "LangChain uses these components to understand and process your request:\n",
    "\n",
    "🧠 1. LLM (Large Language Model)\n",
    "Understands the intent (\"check leave balance\")\n",
    "\n",
    "Recognizes that \"123\" is the employee ID, even without a special format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138f397",
   "metadata": {},
   "source": [
    "🤖 2. Agent\n",
    "Decides which tool/function to use (e.g., check_leave_balance)\n",
    "\n",
    "Based on the tool’s description, matches the query to the correct function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed65c8fb",
   "metadata": {},
   "source": [
    "🛠️ 3. Tool Calling\n",
    "LLM fills in the function arguments like this:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "{\n",
    "  \"name\": \"check_leave_balance\",\n",
    "  \"arguments\": {\n",
    "    \"employee_id\": \"123\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe89ffb",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "1. Employee enters a question.\n",
    "\n",
    "2. LLM analyzes and understands the question.\n",
    "\n",
    "3. Agent decides if a tool needs to be used.\n",
    "\n",
    "4. LLM extracts necessary parameters (like employee ID).\n",
    "\n",
    "5. Agent selects the correct tool based on the question and tool descriptions.\n",
    "\n",
    "6. Tool is called with extracted parameters.\n",
    "\n",
    "7. Tool returns the result.\n",
    "\n",
    "8. LLM uses the result to generate a natural language answer.\n",
    "\n",
    "9. Assistant sends the final response to the employee."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
